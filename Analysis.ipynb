{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('train_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_countvec/train_countvec_features_actor_1_name.npy\n",
      "2063\n",
      "features_countvec/train_countvec_features_actor_2_name.npy\n",
      "2919\n",
      "features_countvec/train_countvec_features_director_name.npy\n",
      "2113\n",
      "features_doc2vec/train_doc2vec_features_genre.npy\n",
      "100\n",
      "features_doc2vec/train_doc2vec_features_plot_keywords.npy\n",
      "100\n",
      "features_fasttext/train_fasttext_title_embeddings.npy\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "names_train = ['features_countvec/train_countvec_features_actor_1_name.npy', 'features_countvec/train_countvec_features_actor_2_name.npy', \n",
    "               'features_countvec/train_countvec_features_director_name.npy', 'features_doc2vec/train_doc2vec_features_genre.npy',\n",
    "               'features_doc2vec/train_doc2vec_features_plot_keywords.npy', 'features_fasttext/train_fasttext_title_embeddings.npy']\n",
    "\n",
    "for name in names_train:\n",
    "    print(name)\n",
    "    print(len(np.load(name)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns(len, unique):\n",
    "    columns = []\n",
    "    for i in range(len):\n",
    "        columns.append(str(i + unique))\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = np.load('features_countvec/train_countvec_features_actor_1_name.npy')\n",
    "df_act_1 = pd.DataFrame(temp1, columns=create_columns(len(temp1[0]), 0))\n",
    "\n",
    "temp2 = np.load('features_countvec/train_countvec_features_actor_2_name.npy')\n",
    "df_act_2 = pd.DataFrame(temp2, columns=create_columns(len(temp2[0]), len(temp1[0])))\n",
    "\n",
    "temp3 = np.load('features_countvec/train_countvec_features_director_name.npy')\n",
    "df_dir = pd.DataFrame(temp3, columns=create_columns(len(temp3[0]), len(temp1[0]) + len(temp2[0])))\n",
    "\n",
    "temp4 = np.load('features_doc2vec/train_doc2vec_features_genre.npy')\n",
    "df_genre = pd.DataFrame(temp4, columns=create_columns(len(temp4[0]), len(temp1[0]) + len(temp2[0]) + 100))\n",
    "\n",
    "temp5 = np.load('features_doc2vec/train_doc2vec_features_plot_keywords.npy')\n",
    "df_keyword = pd.DataFrame(temp5, columns=create_columns(len(temp5[0]), len(temp1[0]) + len(temp2[0]) + 200))\n",
    "\n",
    "temp6 = np.load('features_fasttext/train_fasttext_title_embeddings.npy')\n",
    "df_title = pd.DataFrame(temp6, columns=create_columns(len(temp6[0]), len(temp1[0]) + len(temp2[0]) + 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_act_1, df_act_2, df_dir, df_genre, df_keyword, df_title]\n",
    "pre_data = pd.concat(df_list, axis=1)\n",
    "\n",
    "data_new = pd.concat([data, pre_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_new.drop(columns='imdb_score_binned'), data_new['imdb_score_binned'], test_size=0.2, random_state=42)\n",
    "#X_train = X_train.reset_index()\n",
    "#X_test = X_test.reset_index()\n",
    "#y_train = y_train.reset_index()\n",
    "#y_test = y_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country\n",
       "USA              1919\n",
       "UK                202\n",
       "France             64\n",
       "Germany            52\n",
       "Canada             36\n",
       "Australia          26\n",
       "Spain              17\n",
       "Hong Kong           9\n",
       "New Zealand         9\n",
       "Japan               6\n",
       "Italy               6\n",
       "Denmark             6\n",
       "China               5\n",
       "South Korea         5\n",
       "Ireland             4\n",
       "Mexico              4\n",
       "South Africa        3\n",
       "Thailand            3\n",
       "Brazil              2\n",
       "Romania             2\n",
       "Norway              2\n",
       "Argentina           2\n",
       "Russia              2\n",
       "India               2\n",
       "Netherlands         2\n",
       "Belgium             1\n",
       "Iran                1\n",
       "Aruba               1\n",
       "Hungary             1\n",
       "Finland             1\n",
       "Poland              1\n",
       "Taiwan              1\n",
       "Official site       1\n",
       "West Germany        1\n",
       "Israel              1\n",
       "Chile               1\n",
       "New Line            1\n",
       "Iceland             1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "language\n",
       "English       2308\n",
       "French          25\n",
       "Spanish         14\n",
       "German           8\n",
       "Mandarin         5\n",
       "Cantonese        5\n",
       "Italian          4\n",
       "Danish           3\n",
       "Japanese         3\n",
       "Thai             2\n",
       "Aboriginal       2\n",
       "Korean           2\n",
       "Portuguese       2\n",
       "Norwegian        2\n",
       "Dutch            2\n",
       "Hindi            2\n",
       "Kazakh           1\n",
       "Maya             1\n",
       "Arabic           1\n",
       "Aramaic          1\n",
       "Hebrew           1\n",
       "Filipino         1\n",
       "Mongolian        1\n",
       "Romanian         1\n",
       "Hungarian        1\n",
       "Zulu             1\n",
       "Bosnian          1\n",
       "Persian          1\n",
       "Dari             1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pre_data = X_train.iloc[:, 26:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA\n",
    "#PCs = [200, 400, 600, 800, 1000, 1500, 2000, 3000]\n",
    "#var = []\n",
    "#for n in PCs:\n",
    "\n",
    "#    pca = PCA(n_components=n)\n",
    "\n",
    "#    df_list = [df_act_1, df_act_2, df_dir, df_genre, df_keyword, df_title]\n",
    "#    train_pre_data = pd.concat(df_list, axis=1)\n",
    "\n",
    "#    columns = []\n",
    "#    for i in range(n):\n",
    "#        columns.append(str(i))\n",
    "\n",
    "#    principalComponents = pca.fit_transform(train_pre_data)\n",
    "\n",
    "#    var.append(np.var(principalComponents))\n",
    "#import matplotlib.pyplot as plt\n",
    "#cum_var = np.cumsum(var)\n",
    "\n",
    "#plt.xlabel('Principal Components')\n",
    "#plt.xlabel('Cumulative Explained Variance')\n",
    "#plt.plot(PCs, cum_var)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement PCA to cut down columns further\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1000)\n",
    "\n",
    "columns = []\n",
    "for i in range(1000):\n",
    "    columns.append(str(i))\n",
    "\n",
    "principalComponents = pca.fit_transform(train_pre_data)\n",
    "\n",
    "df_pca = pd.DataFrame(data=principalComponents, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "selector = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=100, step=10)\n",
    "X_train_RFE = selector.fit_transform(df_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_selection import RFECV\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#selector = RFECV(estimator=DecisionTreeClassifier(), step=10, cv=5)\n",
    "#selector.fit(df_pca, train['imdb_score_binned'])\n",
    "\n",
    "\n",
    "#print(selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i in range(100):\n",
    "    columns.append(str(i))\n",
    "\n",
    "df_selected_train = pd.DataFrame(data=X_train_RFE, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old columns and unnecessary columns\n",
    "titles = ['actor_1_name', 'actor_2_name', 'director_name', 'genres', 'plot_keywords', 'language', 'country', 'id',\n",
    "            'title_embedding', 'movie_title', 'actor_3_name', 'actor_3_facebook_likes']\n",
    "df_temp = X_train.iloc[:, :26].drop(columns=titles)\n",
    "train_new = pd.concat([df_temp.reset_index(), df_selected_train.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content_rating\n",
       "R           1102\n",
       "PG-13        842\n",
       "PG           355\n",
       "G             53\n",
       "Unrated       31\n",
       "Approved      11\n",
       "X              9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['content_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content_rating\n",
       "R           1102\n",
       "PG-13        842\n",
       "PG           355\n",
       "G             53\n",
       "Unrated       31\n",
       "Approved      11\n",
       "X              9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine ratings\n",
    "X_train.loc[X_train.content_rating == 'GP', 'content_rating'] = 'G'\n",
    "X_train.loc[X_train.content_rating == 'Passed', 'content_rating'] = 'Approved'\n",
    "X_train.loc[X_train.content_rating == 'NC-17', 'content_rating'] = 'X'\n",
    "X_train.loc[X_train.content_rating == 'Not Rated', 'content_rating'] = 'Unrated'\n",
    "X_train.loc[X_train.content_rating == 'M', 'content_rating'] = 'PG-13'\n",
    "\n",
    "X_train['content_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "encoded = enc.fit_transform(X_train[['content_rating']])\n",
    "\n",
    "df_temp = pd.DataFrame(encoded.toarray(), columns=['Approved', 'G', 'PG', 'PG-13', 'R', 'Unrated', 'X'])\n",
    "\n",
    "train_final = pd.concat([train_new.drop(columns=['content_rating']), df_temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply post processing, ie normalisation, standardisation\n",
    "# Remove outliers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pre_data = X_test.iloc[:, 26:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fitted PCA to test data\n",
    "test_pcs = pca.transform(test_pre_data)\n",
    "\n",
    "columns = []\n",
    "for i in range(1000):\n",
    "    columns.append(str(i))\n",
    "\n",
    "df_pca_test = pd.DataFrame(data=test_pcs, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_RFE = selector.transform(df_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i in range(100):\n",
    "    columns.append(str(i))\n",
    "\n",
    "df_selected_test = pd.DataFrame(data=X_test_RFE, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    }
   ],
   "source": [
    "test_new = pd.concat([X_test.iloc[:, :26].drop(columns=titles).reset_index(), df_selected_test.reset_index()], axis=1)\n",
    "\n",
    "print(len(test_new.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ratings\n",
    "X_test.loc[X_test.content_rating == 'GP', 'content_rating'] = 'G'\n",
    "X_test.loc[X_test.content_rating == 'Passed', 'content_rating'] = 'Approved'\n",
    "X_test.loc[X_test.content_rating == 'NC-17', 'content_rating'] = 'X'\n",
    "X_test.loc[X_test.content_rating == 'Not Rated', 'content_rating'] = 'Unrated'\n",
    "X_test.loc[X_test.content_rating == 'M', 'content_rating'] = 'PG-13'\n",
    "\n",
    "encoded = enc.transform(X_test[['content_rating']])\n",
    "\n",
    "df_temp = pd.DataFrame(encoded.toarray(), columns=['Approved', 'G', 'PG', 'PG-13', 'R', 'Unrated', 'X'])\n",
    "\n",
    "test_new = pd.concat([test_new.drop(columns='content_rating'), df_temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_test = scaler.transform(test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imdb_score_binned\n",
       "2    487\n",
       "3    100\n",
       "4     14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GTB = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1,\n",
    "                                 max_depth=5, random_state=0).fit(X_norm, y_train)\n",
    "y_GTB = GTB.predict(X_norm_test)\n",
    "df_GTB = pd.DataFrame(y_GTB, columns=['imdb_score_binned'])\n",
    "df_GTB.insert(0, 'id', range(1, len(df_GTB) + 1))\n",
    "df_GTB['imdb_score_binned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7021630615640599"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_GTB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
