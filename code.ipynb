{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('train_dataset.csv')\n",
    "test = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_countvec/train_countvec_features_actor_1_name.npy\n",
      "2063\n",
      "features_countvec/train_countvec_features_actor_2_name.npy\n",
      "2919\n",
      "features_countvec/train_countvec_features_director_name.npy\n",
      "2113\n",
      "features_doc2vec/train_doc2vec_features_genre.npy\n",
      "100\n",
      "features_doc2vec/train_doc2vec_features_plot_keywords.npy\n",
      "100\n",
      "features_fasttext/train_fasttext_title_embeddings.npy\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "names_train = ['features_countvec/train_countvec_features_actor_1_name.npy', 'features_countvec/train_countvec_features_actor_2_name.npy', \n",
    "               'features_countvec/train_countvec_features_director_name.npy', 'features_doc2vec/train_doc2vec_features_genre.npy',\n",
    "               'features_doc2vec/train_doc2vec_features_plot_keywords.npy', 'features_fasttext/train_fasttext_title_embeddings.npy']\n",
    "names_test = ['features_countvec/test_countvec_features_actor_1_name.npy', 'features_countvec/test_countvec_features_actor_2_name.npy', \n",
    "               'features_countvec/test_countvec_features_director_name.npy', 'features_doc2vec/test_doc2vec_features_genre.npy',\n",
    "               'features_doc2vec/test_doc2vec_features_plot_keywords.npy', 'features_fasttext/test_fasttext_title_embeddings.npy']\n",
    "\n",
    "for name in names_train:\n",
    "    print(name)\n",
    "    print(len(np.load(name)[0]))\n",
    "\n",
    "# The count vectors have too many columns and need to be trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_columns(len, unique):\n",
    "    columns = []\n",
    "    for i in range(len):\n",
    "        columns.append(str(i + unique))\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = np.load('features_countvec/train_countvec_features_actor_1_name.npy')\n",
    "df_act_1 = pd.DataFrame(temp1, columns=create_columns(len(temp1[0]), 0))\n",
    "\n",
    "temp2 = np.load('features_countvec/train_countvec_features_actor_2_name.npy')\n",
    "df_act_2 = pd.DataFrame(temp2, columns=create_columns(len(temp2[0]), len(temp1[0])))\n",
    "\n",
    "temp3 = np.load('features_countvec/train_countvec_features_director_name.npy')\n",
    "df_dir = pd.DataFrame(temp3, columns=create_columns(len(temp3[0]), len(temp1[0]) + len(temp2[0])))\n",
    "\n",
    "temp4 = np.load('features_doc2vec/train_doc2vec_features_genre.npy')\n",
    "df_genre = pd.DataFrame(temp4, columns=create_columns(len(temp4[0]), len(temp1[0]) + len(temp2[0]) + 100))\n",
    "\n",
    "temp5 = np.load('features_doc2vec/train_doc2vec_features_plot_keywords.npy')\n",
    "df_keyword = pd.DataFrame(temp5, columns=create_columns(len(temp5[0]), len(temp1[0]) + len(temp2[0]) + 200))\n",
    "\n",
    "temp6 = np.load('features_fasttext/train_fasttext_title_embeddings.npy')\n",
    "df_title = pd.DataFrame(temp6, columns=create_columns(len(temp6[0]), len(temp1[0]) + len(temp2[0]) + 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.040870</td>\n",
       "      <td>-0.020972</td>\n",
       "      <td>-0.000291</td>\n",
       "      <td>-0.019807</td>\n",
       "      <td>-0.067842</td>\n",
       "      <td>-0.039268</td>\n",
       "      <td>0.021977</td>\n",
       "      <td>0.009322</td>\n",
       "      <td>-0.022308</td>\n",
       "      <td>-0.052090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022151</td>\n",
       "      <td>0.062721</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.009476</td>\n",
       "      <td>-0.024666</td>\n",
       "      <td>-0.011129</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.012796</td>\n",
       "      <td>0.023309</td>\n",
       "      <td>-0.006035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.033925</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>-0.035483</td>\n",
       "      <td>0.005658</td>\n",
       "      <td>-0.060051</td>\n",
       "      <td>-0.015004</td>\n",
       "      <td>-0.044098</td>\n",
       "      <td>-0.028775</td>\n",
       "      <td>0.066608</td>\n",
       "      <td>-0.019120</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.073046</td>\n",
       "      <td>0.038297</td>\n",
       "      <td>-0.044502</td>\n",
       "      <td>-0.027140</td>\n",
       "      <td>0.041589</td>\n",
       "      <td>0.113115</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>-0.070862</td>\n",
       "      <td>-0.003725</td>\n",
       "      <td>-0.001442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.107427</td>\n",
       "      <td>0.041082</td>\n",
       "      <td>1.065319</td>\n",
       "      <td>0.087488</td>\n",
       "      <td>0.233568</td>\n",
       "      <td>-0.097163</td>\n",
       "      <td>-0.085989</td>\n",
       "      <td>-0.001578</td>\n",
       "      <td>0.078709</td>\n",
       "      <td>-0.038435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021590</td>\n",
       "      <td>-0.036569</td>\n",
       "      <td>-0.066415</td>\n",
       "      <td>-0.018222</td>\n",
       "      <td>-0.038458</td>\n",
       "      <td>-0.053032</td>\n",
       "      <td>0.004183</td>\n",
       "      <td>0.052738</td>\n",
       "      <td>0.036958</td>\n",
       "      <td>-0.009681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.954659</td>\n",
       "      <td>-0.382909</td>\n",
       "      <td>0.102390</td>\n",
       "      <td>-0.118210</td>\n",
       "      <td>-0.026847</td>\n",
       "      <td>-0.015419</td>\n",
       "      <td>-0.038244</td>\n",
       "      <td>-0.019050</td>\n",
       "      <td>0.041273</td>\n",
       "      <td>0.030041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009866</td>\n",
       "      <td>0.021702</td>\n",
       "      <td>-0.072854</td>\n",
       "      <td>0.023565</td>\n",
       "      <td>0.025225</td>\n",
       "      <td>-0.001840</td>\n",
       "      <td>-0.007941</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0.025532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039269</td>\n",
       "      <td>-0.068133</td>\n",
       "      <td>-0.053963</td>\n",
       "      <td>-0.056048</td>\n",
       "      <td>-0.171098</td>\n",
       "      <td>-0.109997</td>\n",
       "      <td>-0.184301</td>\n",
       "      <td>-0.695090</td>\n",
       "      <td>-0.328191</td>\n",
       "      <td>0.731292</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000423</td>\n",
       "      <td>-0.005158</td>\n",
       "      <td>-0.019132</td>\n",
       "      <td>-0.006927</td>\n",
       "      <td>-0.016068</td>\n",
       "      <td>-0.016270</td>\n",
       "      <td>-0.005062</td>\n",
       "      <td>0.026129</td>\n",
       "      <td>-0.033984</td>\n",
       "      <td>-0.041692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.040870 -0.020972 -0.000291 -0.019807 -0.067842 -0.039268  0.021977   \n",
       "1 -0.033925  0.020134 -0.035483  0.005658 -0.060051 -0.015004 -0.044098   \n",
       "2 -0.107427  0.041082  1.065319  0.087488  0.233568 -0.097163 -0.085989   \n",
       "3  0.954659 -0.382909  0.102390 -0.118210 -0.026847 -0.015419 -0.038244   \n",
       "4 -0.039269 -0.068133 -0.053963 -0.056048 -0.171098 -0.109997 -0.184301   \n",
       "\n",
       "          7         8         9  ...       990       991       992       993  \\\n",
       "0  0.009322 -0.022308 -0.052090  ...  0.022151  0.062721  0.002703  0.009476   \n",
       "1 -0.028775  0.066608 -0.019120  ... -0.073046  0.038297 -0.044502 -0.027140   \n",
       "2 -0.001578  0.078709 -0.038435  ... -0.021590 -0.036569 -0.066415 -0.018222   \n",
       "3 -0.019050  0.041273  0.030041  ...  0.009866  0.021702 -0.072854  0.023565   \n",
       "4 -0.695090 -0.328191  0.731292  ... -0.000423 -0.005158 -0.019132 -0.006927   \n",
       "\n",
       "        994       995       996       997       998       999  \n",
       "0 -0.024666 -0.011129 -0.000008  0.012796  0.023309 -0.006035  \n",
       "1  0.041589  0.113115  0.003074 -0.070862 -0.003725 -0.001442  \n",
       "2 -0.038458 -0.053032  0.004183  0.052738  0.036958 -0.009681  \n",
       "3  0.025225 -0.001840 -0.007941  0.004382  0.001915  0.025532  \n",
       "4 -0.016068 -0.016270 -0.005062  0.026129 -0.033984 -0.041692  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement PCA to cut down columns further\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1000)\n",
    "\n",
    "df_list = [df_act_1, df_act_2, df_dir, df_genre, df_keyword, df_title]\n",
    "train_pre_data = pd.concat(df_list, axis=1)\n",
    "\n",
    "columns = []\n",
    "for i in range(1000):\n",
    "    columns.append(str(i))\n",
    "\n",
    "principalComponents = pca.fit_transform(train_pre_data)\n",
    "\n",
    "df_pca = pd.DataFrame(data=principalComponents, columns=columns)\n",
    "\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "selector = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=100, step=10)\n",
    "test_x = selector.fit_transform(df_pca, train['imdb_score_binned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i in range(100):\n",
    "    columns.append(str(i))\n",
    "\n",
    "df_selected_train = pd.DataFrame(data=test_x, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "# Drop old columns and unnecessary columns\n",
    "titles = ['actor_1_name', 'actor_2_name', 'director_name', 'genres', 'plot_keywords', 'language', 'country', 'id',\n",
    "            'title_embedding', 'movie_title', 'actor_3_name', 'actor_3_facebook_likes']\n",
    "train_new = pd.concat([train.drop(columns=titles), df_selected_train], axis=1)\n",
    "\n",
    "print(len(train_new.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content_rating\n",
       "R            1362\n",
       "PG-13        1039\n",
       "PG            458\n",
       "G              68\n",
       "Not Rated      27\n",
       "Unrated        19\n",
       "Approved       14\n",
       "X               9\n",
       "NC-17           3\n",
       "M               2\n",
       "Passed          2\n",
       "GP              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use OHE\n",
    "train['content_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "content_rating\n",
       "R           1362\n",
       "PG-13       1041\n",
       "PG           458\n",
       "G             69\n",
       "Unrated       46\n",
       "Approved      16\n",
       "X             12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine ratings\n",
    "train.loc[train.content_rating == 'GP', 'content_rating'] = 'G'\n",
    "train.loc[train.content_rating == 'Passed', 'content_rating'] = 'Approved'\n",
    "train.loc[train.content_rating == 'NC-17', 'content_rating'] = 'X'\n",
    "train.loc[train.content_rating == 'Not Rated', 'content_rating'] = 'Unrated'\n",
    "train.loc[train.content_rating == 'M', 'content_rating'] = 'PG-13'\n",
    "\n",
    "train['content_rating'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "encoded = enc.fit_transform(train[['content_rating']])\n",
    "\n",
    "df_temp = pd.DataFrame(encoded.toarray(), columns=['Approved', 'G', 'PG', 'PG-13', 'R', 'Unrated', 'X'])\n",
    "\n",
    "train_final = pd.concat([train_new.drop(columns=['content_rating']), df_temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_final['imdb_score_binned']\n",
    "X = train_final.drop(columns=['imdb_score_binned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply post processing, ie normalisation, standardisation\n",
    "# Remove outliers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data\n",
    "temp1_test = np.load('features_countvec/test_countvec_features_actor_1_name.npy')\n",
    "df_act_1_test = pd.DataFrame(temp1_test, columns=create_columns(len(temp1_test[0]), 0))\n",
    "\n",
    "temp2_test = np.load('features_countvec/test_countvec_features_actor_2_name.npy')\n",
    "df_act_2_test = pd.DataFrame(temp2_test, columns=create_columns(len(temp2_test[0]), len(temp1_test[0])))\n",
    "\n",
    "temp3_test = np.load('features_countvec/test_countvec_features_director_name.npy')\n",
    "df_dir_test = pd.DataFrame(temp3_test, columns=create_columns(len(temp3_test[0]), len(temp1_test[0]) + len(temp2_test[0])))\n",
    "\n",
    "temp4_test = np.load('features_doc2vec/test_doc2vec_features_genre.npy')\n",
    "df_genre_test = pd.DataFrame(temp4_test, columns=create_columns(len(temp4_test[0]), len(temp1_test[0]) + len(temp2_test[0]) + 100))\n",
    "\n",
    "temp5_test = np.load('features_doc2vec/test_doc2vec_features_plot_keywords.npy')\n",
    "df_keyword_test = pd.DataFrame(temp5_test, columns=create_columns(len(temp5_test[0]), len(temp1_test[0]) + len(temp2_test[0]) + 200))\n",
    "\n",
    "temp6_test = np.load('features_fasttext/test_fasttext_title_embeddings.npy')\n",
    "df_title_test = pd.DataFrame(temp6_test, columns=create_columns(len(temp6_test[0]), len(temp1_test[0]) + len(temp2_test[0]) + 300))\n",
    "\n",
    "#df_act_1_test = drop_col(df_act_1_test)\n",
    "#df_act_2_test = drop_col(df_act_2_test)\n",
    "#df_dir_test = drop_col(df_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [df_act_1_test, df_act_2_test, df_dir_test, df_genre_test, df_keyword_test, df_title_test]\n",
    "test_pre_data = pd.concat(df_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fitted PCA to test data\n",
    "test_pcs = pca.transform(test_pre_data)\n",
    "\n",
    "columns = []\n",
    "for i in range(1000):\n",
    "    columns.append(str(i))\n",
    "\n",
    "df_pca_test = pd.DataFrame(data=test_pcs, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_100 = selector.transform(df_pca_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for i in range(100):\n",
    "    columns.append(str(i))\n",
    "\n",
    "df_selected_test = pd.DataFrame(data=x_test_100, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "test_new = pd.concat([test.drop(columns=titles), df_selected_test], axis=1)\n",
    "\n",
    "print(len(test_new.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['content_rating'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[624], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m encoded \u001b[38;5;241m=\u001b[39m enc\u001b[38;5;241m.\u001b[39mtransform(test[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_rating\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     14\u001b[0m df_temp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(encoded\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApproved\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPG-13\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrated\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 16\u001b[0m test_new \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([test_new\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_rating\u001b[39m\u001b[38;5;124m'\u001b[39m), df_temp], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:5568\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5421\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5422\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5429\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5430\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5432\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5433\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5566\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5567\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[1;32m   5569\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[1;32m   5570\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   5571\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[1;32m   5572\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m   5573\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   5574\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[1;32m   5575\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m   5576\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4785\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4783\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4785\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4788\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:4827\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4825\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4827\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4828\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4830\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4831\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['content_rating'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Remove low frequency ratings\n",
    "#count = test_new['content_rating'].value_counts()\n",
    "#test_new = test_new[~test_new['content_rating'].isin(count[count < 15].index)]  \n",
    "\n",
    "# Combine ratings\n",
    "test.loc[test.content_rating == 'GP', 'content_rating'] = 'G'\n",
    "test.loc[test.content_rating == 'Passed', 'content_rating'] = 'Approved'\n",
    "test.loc[test.content_rating == 'NC-17', 'content_rating'] = 'X'\n",
    "test.loc[test.content_rating == 'Not Rated', 'content_rating'] = 'Unrated'\n",
    "test.loc[test.content_rating == 'M', 'content_rating'] = 'PG-13'\n",
    "\n",
    "encoded = enc.transform(test[['content_rating']])\n",
    "\n",
    "df_temp = pd.DataFrame(encoded.toarray(), columns=['Approved', 'G', 'PG', 'PG-13', 'R', 'Unrated', 'X'])\n",
    "\n",
    "test_new = pd.concat([test_new.drop(columns='content_rating'), df_temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_test = scaler.transform(test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features that were hurting our model\n",
    "remove_negative_features = ['num_critic_for_reviews', 'director_facebook_likes', 'actor_1_facebook_likes', 'facenumber_in_poster', \n",
    "                            'average_degree_centrality', '0', '1', '3', '5', '6', '9', '10', '12', '13', '14', '15', \n",
    "                            '16', '17', '19', '21', '23', '24', '25', '27', '28', '29', '30', '31', '33', '36', '37', '38', '44', \n",
    "                            '47', '48', '50', '51', '54', '55', '58', '62', '63', '64', '66', '68', '72', '73', '74', '76', '77', \n",
    "                            '79', '81', '82', '85', '88', '89', '90', '92', '93', '95', '96', '98', 'G', 'PG', 'R']\n",
    "\n",
    "X_norm_test_dropped = pd.DataFrame(X_norm_test, columns=test_new.columns).drop(columns=remove_negative_features)\n",
    "\n",
    "X_norm_dropped = pd.DataFrame(X_norm, columns=test_new.columns).drop(columns=remove_negative_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "imdb_score_binned\n",
       "2    586\n",
       "3    129\n",
       "4     24\n",
       "1     12\n",
       "0      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GTB = GradientBoostingClassifier(n_estimators=100, learning_rate=0.3,\n",
    "                                 max_depth=5, random_state=0).fit(X_norm_dropped, y)\n",
    "y_GTB = GTB.predict(X_norm_test_dropped)\n",
    "df_GTB = pd.DataFrame(y_GTB, columns=['imdb_score_binned'])\n",
    "df_GTB.insert(0, 'id', range(1, len(df_GTB) + 1))\n",
    "df_GTB['imdb_score_binned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GTB.to_csv('submission.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
